# 大模型知识增强策略

## 1 为什么要用知识图谱去增强预训练模型？

我们希望预训练模型在需要特定知识或常识的任务中表现更佳。比如，在机器阅读理解中，当给定一句《哈利波特》小说中的句子：“Harry Potter points his wand at Lord Voldemort.”时，模型需要理解哈利波特和伏地魔之间的时空关系，这就需要模型有一定的《哈利波特》先验知识，而不只是将它们视为简单的称谓（指代）。



在文本生成中，例如使用“河、鱼、网、捉”这几个词造句，我们希望生成的句子至少要符合正常的逻辑关系，比如“人在河里用网捉鱼”，而不是“鱼在捉网”等不合理的句子。这需要模型具备一定的常识。



尽管通过大量的文本预训练，这些常识可能已经隐式地存储在模型中，但这些知识并不足以让预训练模型处理特定任务。因此，我们需要对模型进行**知识的注入和增强**。另一方面，通过增强模型的知识，预训练模型可以更像是领域专家或生活中的幽默大师。比如，在与对话机器人交谈时，你说：“翻译翻译，什么叫惊喜。”你更期望它将“惊喜”翻译成英文，还是希望它调侃一下，玩一些幽默梗呢？

---

## 2 知识图谱与预训练模型结合时的常见问题

当将知识图谱信息注入预训练模型时，通常会面临三个主要问题：

1. **结构化信息的非结构化**：知识图谱以**三元组列表或有向图**的形式呈现，相较于非结构化的自然语言文本，蕴含了更多的**结构化信息**。将这样的结构化信息与预训练模型结合起来是第一步，需要解决如何有效地整合这种结构化信息。
2. **异构特征空间的对齐**：预训练模型中的**token特征**与知识图谱中的**embedding**通常来自不同的方法，导致它们处于不同的特征空间。为了将它们整合起来，需要学习一个线性变换来对齐它们的特征空间。
3. **知识噪声的解决**：如果无法有效地融合知识图谱信息，可能会降低预训练模型的性能。在嵌入层面，知识信息可能会干扰注意力机制的运算，导致**推理与模型参数之间的矛盾**；在输入文本上，知识噪声可能会破坏原有句子结构和表达，影响模型的理解和生成。

---

## 3 相关工作

从知识图谱与预训练模型的增强方式来看，有通过线性变化、注意力机制等在知识实体与关系的嵌入式表达上进行知识融合的，也有直接使用实体描述，设计特定的预训练模型输入。在面向的任务上，除去GLUE等进行通用自然语言理解测试的benchmark之外，还包括近期热门的知识增强的开放领域问答、常识文本生成等等。



由于相关工作过于庞杂，本文从模型的角度入手，按照知识与预训练模型融合的方式进行分类，仅仅选取具有代表性的工作进行重点分析。粗略划分，知识融合的方法可简单分成三类：**embedding层面上的融合**，**token层面的融合**，以及**知识图谱与预训练模型的共同学习**。

----

### **3.1 隐式融合——使用embedding在模型内部融合**

隐式融合是比较直接的embedding融合方法，该类方法基于一些KGE（Knowledge Graph Embedding，使用最多的是TransE）算法**获得知识图谱中的实体与关系的embedding**，并为这些**embedding修改预训练模型结构**，以便将二者进行结合。



根据将embedding与预训练模型结合的方式，可以粗略分为**基于projection的结合方法**和**基于attention的结合方法**。

#### **ERNIE**



#### **KnowBERT**



#### **KG-BART**



当然，由于神经网络的可学习性，有时候模型结构对使用效果的影响并不是唯一的。用知识去增强预训练模型，同模型的输入、训练的任务都息息相关。面向特定的下游任务，不同的融合方式甚至也会产生不同的效果。



尽管embedding的方式比较直观，且属于典型的深度学习风格，但该类方法仍然存在以下几类问题：



1. 知识的融合效果受knowledge embedding学习限制；

2. 简单的变换未必能将知识空间与文本空间对齐；

3. 该结构需要对预训练模型重新进行预训练，即使通用的KG+PTM预训练是可行的，针对特定领域的预训练或微调同样存在困难。

----

### **3.2 显式融合——不改变模型结构的融合方式**

与之前所述的基于embedding的结合相比，另一种思路更为直接：既然知识图谱本身就是借助自然语言表达的，能不能直接把实体和关系经过某些变换后，**以token的形式输入到预训练模型中**？这样就不需要再去为每个实体学习embedding，更不需要考虑两种特征空间的聚合。



一些相关的工作都是在这种思路的基础上进行的，这类工作大概可以分成两类：

（1）训练时的知识增强；

（2）推理时的知识增强；



#### **K-BERT**



#### **CoLAKE**





----

### **3.3 知识图谱与预训练模型的共同学习**

除去上述两部分的工作之外，还有一类工作进行了额外的尝试。该类工作并不满足于单纯地将知识图谱增强于预训练模型，而是将知识图谱表示学习和面向知识的自然语言理解一起处理，并希望通过这种方式可以在这两个领域内都得到模型的提升。



#### **KEPLER**



#### **JAKET**





----

